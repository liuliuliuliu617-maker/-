import os
import json
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
import re
import time
import argparse

# ============================ é…ç½®å‚æ•° ============================

# 1. è·¯å¾„é…ç½®
BASE_MODEL_PATH = r"models\Qwen2.5-3B-Instruct-Local"

# å¯é€‰æ¿å—ä»¥åŠå¯¹åº”çš„ LoRA æ¨¡å‹ç›®å½•ï¼ˆä½äºå½“å‰æ–‡ä»¶å¤¹ä¸‹ models/ï¼‰
SECTOR_NAMES = [
    "APEX",
    "cos",
    "FPS",
    "PUBG",
    "å†å²å“é‰´",
    "ä¸‰è§’æ´²",
    "æ—¶æ”¿",
    "è‹±é›„è”ç›Ÿ",
]
DEFAULT_SECTOR = "è‹±é›„è”ç›Ÿ"
SECTOR_MODEL_MAP = {name: os.path.join("models", name) for name in SECTOR_NAMES}

# è¾“å…¥æ–‡ä»¶å¤¹çš„åŸºç¡€è·¯å¾„ (ä½ åªéœ€è¦è¾“å…¥æ–‡ä»¶åï¼Œä»£ç ä¼šè‡ªåŠ¨æ‹¼ä¸Šè¿™ä¸ªè·¯å¾„)
BASE_INPUT_DIR = os.path.join("output", "èˆªç©ºæ¯èˆ°")

# è¾“å‡ºæ–‡ä»¶å¤¹
OUTPUT_DIR = "èˆªç©ºæ¯èˆ°_æœ€ç»ˆåˆ†ç±»ç»“æœ"

# 2. æ€§èƒ½å‚æ•°
# æ‰¹å¤§å°ä¸è¦å¤ªå¤§ï¼Œé¿å…åœ¨ CPU ä¸Šä¸€æ¬¡ç®—å¤ªå¤šå¯¼è‡´éå¸¸æ…¢
BATCH_SIZE = 6
# Windows ç¯å¢ƒä¸‹å…ˆå…³é—­ 4bit é‡åŒ–ï¼Œé¿å…ä¾èµ– bitsandbytes
USE_4BIT = True 

# å…³é”®è¯è§„åˆ™
KEYWORD_RULES = {
    3: ['åŠ å¾®ä¿¡', 'vx', 'qç¾¤', 'ç§ä¿¡æˆ‘', 'ä¸»é¡µæœ‰', 'è¿›ç¾¤', 'è”ç³»æ–¹å¼', 'é€ç¦åˆ©', 'ä¼˜æƒ åˆ¸', 'ç§’æ€', 'é™æ—¶', 'å…è´¹é¢†', 'æ­å­', 'äº’å…³', 'BV'],
    4: ['@'],
    2: ['æ´—æ¾¡ç‹—', 'gsl', 'çŒªæ‚', 'çš‡å†›', 'åƒµå°¸', 'sgjj', 'æ°´é¬¼', 'Ã·', 'å‡ºç”Ÿ', 'æ­»å¦ˆ', 'æ‚äº¤', 'ç›—ç‰ˆ', 'è™šç©º'],
    5: ['ç»éªŒ+3', 'æ°´è´´', 'æ’çœ¼', 'væˆ‘50']
}

TEXT_KEY = "text"
ID_MAP = {1: "æ­£å¸¸", 2: "äº‰è®º", 3: "å¹¿å‘Š", 4: "@æŸäºº", 5: "æ— æ„ä¹‰"}

# ============================ æ ¸å¿ƒå‡½æ•° ============================

def load_model(sector: str | None = None):
    """æ ¹æ®æ¿å—åŠ è½½å¯¹åº”çš„ LoRA æ¨¡å‹ã€‚

    æ¨¡å‹ç›®å½•çº¦å®šä¸º models/<æ¿å—å>ï¼Œä¾‹å¦‚ models/è‹±é›„è”ç›Ÿã€‚
    å½“ä¼ å…¥æœªçŸ¥æ¿å—æˆ–å¯¹åº” LoRA ç›®å½•ä¸å­˜åœ¨æ—¶ï¼Œä¼šå›é€€åˆ°é»˜è®¤æ¿å— DEFAULT_SECTORã€‚
    """

    original_sector = sector
    # å…ˆæŒ‰åç§°æ£€æŸ¥
    if sector not in SECTOR_MODEL_MAP:
        sector = DEFAULT_SECTOR

    lora_path = SECTOR_MODEL_MAP[sector]

    # å¦‚æœå¯¹åº”ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™å›é€€åˆ°é»˜è®¤æ¿å—
    if not os.path.isdir(lora_path):
        print(
            f"è­¦å‘Š: æ¿å— '{original_sector}' çš„ LoRA è·¯å¾„ '{lora_path}' ä¸å­˜åœ¨ï¼Œ"
            f"å°†ä½¿ç”¨é»˜è®¤æ¿å— '{DEFAULT_SECTOR}' çš„æ¨¡å‹ã€‚"
        )
        sector = DEFAULT_SECTOR
        lora_path = SECTOR_MODEL_MAP[sector]

    print(f"å½“å‰å®é™…ä½¿ç”¨æ¿å—: {sector}")
    print(f"Loading tokenizer from: {BASE_MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left" 

    print(f"Loading model (4-bit={USE_4BIT})...")
    bnb_config = None
    if USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

    # ä¸ºé¿å…è‡ªåŠ¨ offload å¸¦æ¥çš„ LoRA é€‚é… KeyErrorï¼Œæˆ‘ä»¬æ˜¾å¼é€‰æ‹©å•ä¸€è®¾å¤‡åŠ è½½
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if device == "cuda" else torch.float32

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        quantization_config=bnb_config,
        device_map=None,
        torch_dtype=dtype,
        trust_remote_code=True
    ).to(device)

    print(f"Loading LoRA adapter from: {lora_path}")
    # åŒæ ·ä¸ä½¿ç”¨ offloadï¼Œç›´æ¥åœ¨åŒä¸€è®¾å¤‡ä¸ŠåŠ è½½ LoRA
    model = PeftModel.from_pretrained(base_model, lora_path, device_map=None)
    model.to(device)
    # å…¼å®¹åç»­ model.device çš„ç”¨æ³•
    model.device = torch.device(device)
    model.eval() 
    
    return model, tokenizer

def check_keywords(text):
    text_lower = text.lower()
    for category_id, keywords in KEYWORD_RULES.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return category_id, keyword
    return None, None

def predict_batch_llm(texts, model, tokenizer):
    prompts = []
    for text in texts:
        prompt = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªåˆ†ç±»åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\nè¯·å¯¹ä»¥ä¸‹è¯„è®ºè¿›è¡Œåˆ†ç±»ï¼Œåªè¾“å‡ºç±»åˆ«IDã€‚\nè¯„è®ºå†…å®¹ï¼š{text}<|im_end|>\n<|im_start|>assistant\n"
        prompts.append(prompt)
    
    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=2,
            temperature=0.1,  
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id
        )
    
    generated_ids = outputs[:, inputs.input_ids.shape[1]:]
    decoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    
    results = []
    for output_text in decoded_outputs:
        match = re.search(r'\d+', output_text)
        if match:
            results.append(int(match.group(0)))
        else:
            results.append(1)
    return results

def classify_items(data, model, tokenizer):
    """å¯¹ä¸€æ‰¹è¯„è®ºæ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œè¿”å›å¸¦é¢„æµ‹ç»“æœçš„åˆ—è¡¨ã€‚

    data: List[dict]ï¼Œæ¯é¡¹è‡³å°‘åŒ…å« TEXT_KEY å¯¹åº”çš„æ–‡æœ¬å­—æ®µï¼Œä¾‹å¦‚ {"bv": ..., "text": ...}
    """
    results = []
    batch_indices = []
    batch_texts = []
    
    start_time = time.time()
    
    # 1. é¢„å¤„ç†ä¸å…³é”®è¯åŒ¹é…
    for i, item in enumerate(data):
        text = item.get(TEXT_KEY, "").strip()
        item['classification_method'] = "empty"
        item['predicted_id'] = None
        
        if not text:
            results.append(item)
            continue
            
        clean_text = text.replace('\n', ' ').replace('\r', '')
        kw_id, hit_word = check_keywords(clean_text)
        
        if kw_id is not None:
            item['predicted_id'] = kw_id
            item['classification_method'] = f"keyword ({hit_word})"
            item['predicted_label'] = ID_MAP.get(kw_id, "æœªçŸ¥")
            results.append(item)
        else:
            batch_indices.append(i)
            batch_texts.append(clean_text)
            results.append(item)
    
    # 2. æ‰¹é‡æ¨¡å‹é¢„æµ‹ï¼ˆå¯¹æ‰€æœ‰éœ€æ¨¡å‹åˆ¤æ–­çš„è¯„è®ºå…¨éƒ¨è°ƒç”¨ LLMï¼‰
    if batch_texts:
        total_to_predict = len(batch_texts)
        texts_for_llm = batch_texts
        indices_for_llm = batch_indices

        total_batches = (total_to_predict + BATCH_SIZE - 1) // BATCH_SIZE
        # æ³¨æ„ï¼šWindows æ§åˆ¶å°é»˜è®¤ä½¿ç”¨ GBK ç¼–ç ï¼Œæ— æ³•æ˜¾ç¤º emojiï¼Œä¼šå¯¼è‡´ UnicodeEncodeError
        print(f"æ­£åœ¨è°ƒç”¨ GPU é¢„æµ‹ {total_to_predict} æ¡è¯„è®º (å…± {total_batches} æ‰¹)...")

        for i in tqdm(range(0, total_to_predict, BATCH_SIZE), leave=False):
            current_texts = texts_for_llm[i : i + BATCH_SIZE]
            current_indices = indices_for_llm[i : i + BATCH_SIZE]

            pred_ids = predict_batch_llm(current_texts, model, tokenizer)

            for idx, pred_id in zip(current_indices, pred_ids):
                results[idx]['predicted_id'] = pred_id
                results[idx]['predicted_label'] = ID_MAP.get(pred_id, "æœªçŸ¥")
                results[idx]['classification_method'] = "model_batch"


    return results


def process_single_file(filename, model, tokenizer):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ï¼ˆäº¤äº’æ¨¡å¼ä¸‹ä½¿ç”¨ BASE_INPUT_DIR/OUTPUT_DIRï¼‰ã€‚"""
    input_file = os.path.join(BASE_INPUT_DIR, filename)
    output_file = os.path.join(OUTPUT_DIR, f"classified_{filename}")

    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯ï¼šåœ¨ '{BASE_INPUT_DIR}' ä¸‹æ‰¾ä¸åˆ°æ–‡ä»¶ '{filename}'")
        return

    print(f"ğŸ“– æ­£åœ¨è¯»å– {filename} ...")
    with open(input_file, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            print("âŒ é”™è¯¯ï¼šJSON æ–‡ä»¶æ ¼å¼æŸåã€‚")
            return

    if not data:
        print("âš ï¸  è­¦å‘Šï¼šæ–‡ä»¶ä¸ºç©ºã€‚")
        return

    start_time = time.time()

    results = classify_items(data, model, tokenizer)

    # 3. ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
        
    elapsed = time.time() - start_time
    print(f"âœ… å¤„ç†å®Œæˆï¼è€—æ—¶: {elapsed:.2f}s")
    print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜è‡³: {output_file}")


# ============================ ä¸»ç¨‹åº (äº¤äº’å¾ªç¯) ============================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¯„è®ºåˆ†ç±»ï¼ˆæŒ‰æ¿å—é€‰æ‹©æ¨¡å‹ï¼‰")
    parser.add_argument(
        "--sector",
        type=str,
        default=None,
        help="æ¿å—åç§°ï¼Œå¦‚: APEX / cos / FPS / PUBG / å†å²å“é‰´ / ä¸‰è§’æ´² / æ—¶æ”¿ / è‹±é›„è”ç›Ÿ",
    )
    parser.add_argument(
        "--in",
        dest="input_path",
        type=str,
        default=None,
        help="è¾“å…¥ JSON è·¯å¾„ï¼ˆæœåŠ¡æ¨¡å¼ï¼Œé…åˆ --out ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--out",
        dest="output_path",
        type=str,
        default=None,
        help="è¾“å‡º JSON è·¯å¾„ï¼ˆæœåŠ¡æ¨¡å¼ï¼Œé…åˆ --in ä½¿ç”¨ï¼‰",
    )

    args = parser.parse_args()

    sector = args.sector
    service_mode = bool(args.input_path and args.output_path)

    # å†³å®šæ¿å—
    if not sector:
        if service_mode:
            # æœåŠ¡æ¨¡å¼ä¸‹æœªæ˜¾å¼æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤æ¿å—
            sector = DEFAULT_SECTOR
        else:
            print("å¯é€‰æ¿å—ï¼š" + " / ".join(SECTOR_NAMES))
            sector_input = input(f"è¯·è¾“å…¥æ¿å—åç§°ï¼ˆé»˜è®¤ {DEFAULT_SECTOR}ï¼‰: ").strip()
            sector = sector_input or DEFAULT_SECTOR

    if sector not in SECTOR_MODEL_MAP:
        print(f"æœªè¯†åˆ«æ¿å— '{sector}'ï¼Œä½¿ç”¨é»˜è®¤ '{DEFAULT_SECTOR}'")
        sector = DEFAULT_SECTOR

    # 0. å‡†å¤‡è¾“å‡ºç›®å½•ï¼ˆä»…äº¤äº’æ¨¡å¼ä½¿ç”¨ï¼‰
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # 1. å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ (åªåšä¸€æ¬¡)
    print("\n" + "="*50)
    print(f"æ­£åœ¨åˆå§‹åŒ– Qwen2.5-3B æ¨¡å‹ï¼ˆæ¿å—ï¼š{sector}ï¼‰ï¼Œè¯·ç¨å€™...")
    print("è¿™å¯èƒ½éœ€è¦ 10-20 ç§’ï¼Œå–å†³äºä½ çš„ç¡¬ç›˜é€Ÿåº¦ã€‚")
    print("="*50 + "\n")
    
    model, tokenizer = load_model(sector)
    
    # æœåŠ¡æ¨¡å¼ï¼šä¾›åç«¯è°ƒç”¨ï¼Œè¯»å– --inï¼Œå†™å…¥ --out
    if service_mode:
        in_path = args.input_path
        out_path = args.output_path

        try:
            with open(in_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–è¾“å…¥ JSON: {e}", file=sys.stderr)
            sys.exit(1)

        if not isinstance(data, list):
            print("âŒ è¾“å…¥ JSON æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›ä¸ºæ•°ç»„", file=sys.stderr)
            sys.exit(1)

        results = classify_items(data, model, tokenizer)

        # è½¬ä¸ºåç«¯æœŸæœ›çš„ç»“æ„ï¼š
        # [{ original_comment_data: <åŸå§‹æ•°æ®+é¢„æµ‹å­—æ®µ>, predicted_label_id, predicted_label_text }, ...]
        out_items = []
        for item in results:
            out_items.append({
                "original_comment_data": item,
                "predicted_label_id": item.get("predicted_id"),
                "predicted_label_text": item.get("predicted_label"),
            })

        out_dir = os.path.dirname(os.path.abspath(out_path))
        if out_dir and not os.path.exists(out_dir):
            os.makedirs(out_dir, exist_ok=True)

        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(out_items, f, ensure_ascii=False, indent=4)

        print(f"âœ… æœåŠ¡æ¨¡å¼å®Œæˆï¼šè¯»å– {len(data)} æ¡è¯„è®ºï¼Œç»“æœå·²ä¿å­˜è‡³ {out_path}")
        sys.exit(0)

    # äº¤äº’æ¨¡å¼ï¼šæ²¿ç”¨åŸæ¥çš„è¡Œä¸º
    print("\n" + "="*50)
    print("ğŸ‰ æ¨¡å‹åŠ è½½å®Œæ¯•ï¼ç³»ç»Ÿå·²å°±ç»ªã€‚")
    print(f"ğŸ“‚ é»˜è®¤è¾“å…¥ç›®å½•: {BASE_INPUT_DIR}")
    print("="*50)

    # 2. è¿›å…¥äº¤äº’å¾ªç¯
    while True:
        print("\n" + "-"*30)
        user_input = input("è¯·è¾“å…¥ JSON æ–‡ä»¶å (ä¾‹å¦‚: 1.json) | è¾“å…¥ q é€€å‡º: ").strip()
        
        if user_input.lower() in ['q', 'quit', 'exit']:
            print("å†è§ï¼")
            break
            
        if not user_input:
            continue
            
        # å®¹é”™ï¼šå¦‚æœä½ ä¸å°å¿ƒè¾“å…¥äº†å…¨è·¯å¾„ï¼Œä»£ç å°è¯•æå–æ–‡ä»¶å
        if os.path.sep in user_input:
            user_input = os.path.basename(user_input)
            
        # å®¹é”™ï¼šå¦‚æœä½ å¿˜äº†åŠ  .json åç¼€
        if not user_input.endswith('.json'):
            user_input += '.json'
            
        process_single_file(user_input, model, tokenizer)
