import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
import re
import time

# ============================ é…ç½®å‚æ•° ============================

# 1. è·¯å¾„é…ç½®
BASE_MODEL_PATH = "Qwen2.5-3B-Instruct-Local"
LORA_PATH = "qwen2.5_3b_qlora_output(æ—¶æ”¿)" 

# è¾“å…¥æ–‡ä»¶å¤¹çš„åŸºç¡€è·¯å¾„ (ä½ åªéœ€è¦è¾“å…¥æ–‡ä»¶åï¼Œä»£ç ä¼šè‡ªåŠ¨æ‹¼ä¸Šè¿™ä¸ªè·¯å¾„)
BASE_INPUT_DIR = os.path.join("output", "èˆªç©ºæ¯èˆ°")

# è¾“å‡ºæ–‡ä»¶å¤¹
OUTPUT_DIR = "èˆªç©ºæ¯èˆ°_æœ€ç»ˆåˆ†ç±»ç»“æœ"

# 2. æ€§èƒ½å‚æ•°
BATCH_SIZE = 16
USE_4BIT = True 

# å…³é”®è¯è§„åˆ™
KEYWORD_RULES = {
    3: ['åŠ å¾®ä¿¡', 'vx', 'qç¾¤', 'ç§ä¿¡æˆ‘', 'ä¸»é¡µæœ‰', 'è¿›ç¾¤', 'è”ç³»æ–¹å¼', 'é€ç¦åˆ©', 'ä¼˜æƒ åˆ¸', 'ç§’æ€', 'é™æ—¶', 'å…è´¹é¢†', 'æ­å­', 'äº’å…³', 'BV'],
    4: ['@'],
    2: ['æ´—æ¾¡ç‹—', 'gsl', 'çŒªæ‚', 'çš‡å†›', 'åƒµå°¸', 'sgjj', 'æ°´é¬¼', 'Ã·', 'å‡ºç”Ÿ', 'æ­»å¦ˆ', 'æ‚äº¤', 'ç›—ç‰ˆ', 'è™šç©º'],
    5: ['ç»éªŒ+3', 'æ°´è´´', 'æ’çœ¼', 'væˆ‘50']
}

TEXT_KEY = "text"
ID_MAP = {1: "æ­£å¸¸", 2: "äº‰è®º", 3: "å¹¿å‘Š", 4: "@æŸäºº", 5: "æ— æ„ä¹‰"}

# ============================ æ ¸å¿ƒå‡½æ•° ============================

def load_model():
    print(f"Loading tokenizer from: {BASE_MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left" 

    print(f"Loading model (4-bit={USE_4BIT})...")
    bnb_config = None
    if USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH, 
        quantization_config=bnb_config,
        device_map="auto", 
        torch_dtype=torch.bfloat16, 
        trust_remote_code=True
    )

    print(f"Loading LoRA adapter from: {LORA_PATH}")
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    model.eval() 
    
    return model, tokenizer

def check_keywords(text):
    text_lower = text.lower()
    for category_id, keywords in KEYWORD_RULES.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return category_id, keyword
    return None, None

def predict_batch_llm(texts, model, tokenizer):
    prompts = []
    for text in texts:
        prompt = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªåˆ†ç±»åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\nè¯·å¯¹ä»¥ä¸‹è¯„è®ºè¿›è¡Œåˆ†ç±»ï¼Œåªè¾“å‡ºç±»åˆ«IDã€‚\nè¯„è®ºå†…å®¹ï¼š{text}<|im_end|>\n<|im_start|>assistant\n"
        prompts.append(prompt)
    
    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=2,
            temperature=0.1,  
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id
        )
    
    generated_ids = outputs[:, inputs.input_ids.shape[1]:]
    decoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    
    results = []
    for output_text in decoded_outputs:
        match = re.search(r'\d+', output_text)
        if match:
            results.append(int(match.group(0)))
        else:
            results.append(1)
    return results

def process_single_file(filename, model, tokenizer):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘"""
    input_file = os.path.join(BASE_INPUT_DIR, filename)
    output_file = os.path.join(OUTPUT_DIR, f"classified_{filename}")

    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯ï¼šåœ¨ '{BASE_INPUT_DIR}' ä¸‹æ‰¾ä¸åˆ°æ–‡ä»¶ '{filename}'")
        return

    print(f"ğŸ“– æ­£åœ¨è¯»å– {filename} ...")
    with open(input_file, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            print("âŒ é”™è¯¯ï¼šJSON æ–‡ä»¶æ ¼å¼æŸåã€‚")
            return

    if not data:
        print("âš ï¸  è­¦å‘Šï¼šæ–‡ä»¶ä¸ºç©ºã€‚")
        return

    results = []
    batch_indices = []
    batch_texts = []
    
    start_time = time.time()
    
    # 1. é¢„å¤„ç†ä¸å…³é”®è¯åŒ¹é…
    for i, item in enumerate(data):
        text = item.get(TEXT_KEY, "").strip()
        item['classification_method'] = "empty"
        item['predicted_id'] = None
        
        if not text:
            results.append(item)
            continue
            
        clean_text = text.replace('\n', ' ').replace('\r', '')
        kw_id, hit_word = check_keywords(clean_text)
        
        if kw_id is not None:
            item['predicted_id'] = kw_id
            item['classification_method'] = f"keyword ({hit_word})"
            item['predicted_label'] = ID_MAP.get(kw_id, "æœªçŸ¥")
            results.append(item)
        else:
            batch_indices.append(i)
            batch_texts.append(clean_text)
            results.append(item)
    
    # 2. æ‰¹é‡æ¨¡å‹é¢„æµ‹
    if batch_texts:
        total_batches = (len(batch_texts) + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨ GPU é¢„æµ‹ {len(batch_texts)} æ¡è¯„è®º (å…± {total_batches} æ‰¹)...")
        
        for i in tqdm(range(0, len(batch_texts), BATCH_SIZE), leave=False):
            current_texts = batch_texts[i : i + BATCH_SIZE]
            current_indices = batch_indices[i : i + BATCH_SIZE]
            
            pred_ids = predict_batch_llm(current_texts, model, tokenizer)
            
            for idx, pred_id in zip(current_indices, pred_ids):
                results[idx]['predicted_id'] = pred_id
                results[idx]['predicted_label'] = ID_MAP.get(pred_id, "æœªçŸ¥")
                results[idx]['classification_method'] = "model_batch"

    # 3. ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
        
    elapsed = time.time() - start_time
    print(f"âœ… å¤„ç†å®Œæˆï¼è€—æ—¶: {elapsed:.2f}s")
    print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜è‡³: {output_file}")


# ============================ ä¸»ç¨‹åº (äº¤äº’å¾ªç¯) ============================
if __name__ == "__main__":
    # 0. å‡†å¤‡è¾“å‡ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # 1. å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ (åªåšä¸€æ¬¡)
    print("\n" + "="*50)
    print("æ­£åœ¨åˆå§‹åŒ– Qwen2.5-3B æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    print("è¿™å¯èƒ½éœ€è¦ 10-20 ç§’ï¼Œå–å†³äºä½ çš„ç¡¬ç›˜é€Ÿåº¦ã€‚")
    print("="*50 + "\n")
    
    model, tokenizer = load_model()
    
    print("\n" + "="*50)
    print("ğŸ‰ æ¨¡å‹åŠ è½½å®Œæ¯•ï¼ç³»ç»Ÿå·²å°±ç»ªã€‚")
    print(f"ğŸ“‚ é»˜è®¤è¾“å…¥ç›®å½•: {BASE_INPUT_DIR}")
    print("="*50)

    # 2. è¿›å…¥äº¤äº’å¾ªç¯
    while True:
        print("\n" + "-"*30)
        user_input = input("è¯·è¾“å…¥ JSON æ–‡ä»¶å (ä¾‹å¦‚: 1.json) | è¾“å…¥ q é€€å‡º: ").strip()
        
        if user_input.lower() in ['q', 'quit', 'exit']:
            print("å†è§ï¼")
            break
            
        if not user_input:
            continue
            
        # å®¹é”™ï¼šå¦‚æœä½ ä¸å°å¿ƒè¾“å…¥äº†å…¨è·¯å¾„ï¼Œä»£ç å°è¯•æå–æ–‡ä»¶å
        if os.path.sep in user_input:
            user_input = os.path.basename(user_input)
            
        # å®¹é”™ï¼šå¦‚æœä½ å¿˜äº†åŠ  .json åç¼€
        if not user_input.endswith('.json'):
            user_input += '.json'
            
        process_single_file(user_input, model, tokenizer)
