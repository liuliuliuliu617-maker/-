import requests
import json
import time
import os
import re
import random
import sys
import argparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def make_session():
    session = requests.Session()

    retries = Retry(
        total=6,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504, 429],
        allowed_methods=["GET"]
    )

    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    return session


session = make_session()

# ============================
# é…ç½®å‚æ•°
# ============================
DOMAIN_SLEEP = 8
REQUEST_TIMEOUT = 30
RETRY_DELAY = 5


# ============================
# 1. åŠ è½½ Selenium cookies
# ============================
def load_cookies():
    try:
        with open("cookies.json", "r", encoding="utf-8") as f:
            cookie_list = json.load(f)
        return "; ".join(f"{c['name']}={c['value']}" for c in cookie_list)
    except Exception:
        # æ—  cookies æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä»å°è¯•å…¬å…±æ¥å£
        return ""


def make_headers(cookie_str):
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.bilibili.com/",
        "Cookie": cookie_str,
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }


# ============================
# 2. å·¥å…·å‡½æ•°ï¼šæå– BV
# ============================
def extract_bv(url):
    m = re.search(r"BV\w+", url)
    return m.group(0) if m else None


# BV â†’ aid
def get_aid_from_bv(bv, headers):
    api = f"https://api.bilibili.com/x/web-interface/view?bvid={bv}"
    try:
        resp = session.get(api, headers=headers, timeout=REQUEST_TIMEOUT, verify=False)
        resp.raise_for_status()
        data = resp.json()
        if data.get("code") == 0:
            return data["data"]["aid"]
        else:
            print(f"âŒâŒâŒâŒ è·å–AIDå¤±è´¥: {data.get('message')}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"âŒâŒâŒâŒ è·å–AIDè¯·æ±‚å¤±è´¥: {e}")
        return None


# ============================
# 3. æå–è¯„è®ºï¼šåªä¿å­˜æ–‡æœ¬å’ŒBVå·ï¼ˆä¿®æ”¹ç‰ˆï¼‰
# ============================
def extract_clean_comment(reply, bv):
    content = reply.get("content", {})

    # åªä¿å­˜è¯„è®ºæ–‡æœ¬
    text = content.get("message", "").strip()

    # åªè¿”å›æœ‰æ–‡æœ¬å†…å®¹çš„è¯„è®º
    if text:
        return {
            "bv": bv,
            "text": text
        }
    return None


# ============================
# 4. è·å–å…¨éƒ¨è¯„è®ºï¼ˆä¿®æ”¹ç‰ˆï¼‰
# ============================
def fetch_all_comments(aid, bv, headers):
    page = 1
    all_comments = []
    max_pages = 100  # è®¾ç½®æœ€å¤§é¡µæ•°é˜²æ­¢æ— é™å¾ªç¯

    while page <= max_pages:
        api = f"https://api.bilibili.com/x/v2/reply?type=1&oid={aid}&pn={page}&ps=20"
        headers["Referer"] = f"https://www.bilibili.com/video/{bv}/"

        try:
            resp = session.get(api, headers=headers, timeout=REQUEST_TIMEOUT, verify=False)
            resp.raise_for_status()

            try:
                data = resp.json()
            except json.JSONDecodeError:
                print("âš  æ¥å£è¿”å›éJSONï¼Œç–‘ä¼¼é£æ§æˆ–cookieså¤±æ•ˆ")
                print(f"å“åº”å†…å®¹: {resp.text[:200]}")
                break

            if data["code"] != 0:
                print(f"âš  æ¥å£è¿”å›é”™è¯¯: {data.get('message')}")
                break

            replies = data["data"].get("replies", [])
            if not replies:
                print(f"âœ… å·²è·å–æ‰€æœ‰è¯„è®ºï¼Œå…± {len(all_comments)} æ¡")
                break

            # è·å–å½“å‰é¡µçš„è¯„è®ºæ•°é‡
            current_page_count = len(replies)
            all_comments.extend(replies)

            print(f"ğŸ“„ ç¬¬ {page} é¡µè·å–åˆ° {current_page_count} æ¡è¯„è®ºï¼Œæ€»è®¡ {len(all_comments)} æ¡")

            page += 1

            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1, 2))

        except requests.exceptions.RequestException as e:
            print(f"âŒâŒâŒâŒ è·å–è¯„è®ºè¯·æ±‚å¤±è´¥: {e}")
            if isinstance(e, requests.exceptions.Timeout):
                print(f"â¸â¸â¸â¸â¸â¸â¸â¸â¸ï¸ è¯·æ±‚è¶…æ—¶ï¼Œç­‰å¾…{RETRY_DELAY}ç§’åç»§ç»­...")
                time.sleep(RETRY_DELAY)
                continue
            else:
                break

    return all_comments


# ============================
# 5. çˆ¬å•ä¸ªè§†é¢‘çš„å…¨éƒ¨è¯„è®ºï¼ˆä¿®æ”¹ç‰ˆï¼‰
# ============================
def scrape_single_video(bv, domain_name, headers):
    time.sleep(random.uniform(1, 3))

    aid = get_aid_from_bv(bv, headers)
    if not aid:
        print(f"âŒâŒâŒâŒ BV è½¬ AID å¤±è´¥ï¼š{bv}")
        return False

    print(f"å¼€å§‹çˆ¬å–è§†é¢‘ï¼š{bv} (aid={aid})")

    # è·å–å…¨éƒ¨è¯„è®º
    raw_comments = fetch_all_comments(aid, bv, headers)

    if not raw_comments:
        print(f"âš  æœªè·å–åˆ°è¯„è®ºï¼š{bv}")
        return False

    # åªä¿å­˜æ–‡æœ¬å’ŒBVå·ï¼Œè¿‡æ»¤æ‰ç©ºæ–‡æœ¬
    clean_comments = []
    for comment in raw_comments:
        clean_comment = extract_clean_comment(comment, bv)
        if clean_comment:
            clean_comments.append(clean_comment)

    save_dir = os.path.join("output", domain_name)
    os.makedirs(save_dir, exist_ok=True)

    out_path = os.path.join(save_dir, f"{bv}.json")

    try:
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(clean_comments, f, ensure_ascii=False, indent=2)
        print(f"âœ” å·²ä¿å­˜ï¼š{out_path} (å…±{len(clean_comments)}æ¡è¯„è®º)")
        return True
    except Exception as e:
        print(f"âŒâŒâŒâŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return False


# ============================
# 6. ä¸»æµç¨‹ï¼šè¾“å…¥é¢†åŸŸåç§°å’ŒURL
# ============================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--url", help="Bç«™è§†é¢‘URL", required=False)
    parser.add_argument("--out", help="è¾“å‡ºJSONè·¯å¾„(å½“æä¾›URLæ—¶ç”Ÿæ•ˆ)", required=False)
    parser.add_argument("--sector", help="åˆ†ç±»/é¢†åŸŸå(å¯é€‰)", required=False)
    args = parser.parse_args()

    cookie_str = load_cookies()
    headers = make_headers(cookie_str)

    # æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    # 1) CLI å‚æ•°æ¨¡å¼ï¼š--url ä¸ --out
    # 2) äº¤äº’æ¨¡å¼ï¼šè¾“å…¥é¢†åŸŸä¸URLï¼Œè¾“å‡ºåˆ° output/<sector>/<bv>.json
    if args.url and args.out:
        url = args.url.strip()
        bv = extract_bv(url)
        if not bv:
            print(f"âŒ æ— æ³•æå–BVï¼š{url}", file=sys.stderr)
            sys.exit(1)
        # å½“æä¾› out æ—¶ï¼Œå°†æ–‡ä»¶å†™åˆ°æŒ‡å®šè·¯å¾„
        sector = args.sector or "default"
        # çˆ¬å–è¯„è®º
        aid = get_aid_from_bv(bv, headers)
        comments_raw = []
        if aid:
            comments_raw = fetch_all_comments(aid, bv, headers)
        # è½¬æ¢ä¸ºç®€æ´æ ¼å¼
        clean_comments = []
        for comment in comments_raw:
            c = extract_clean_comment(comment, bv)
            if c:
                clean_comments.append(c)
        # è‹¥å› é£æ§æˆ–æ— cookieså¯¼è‡´å¤±è´¥ï¼Œè¾“å‡ºä¸€ä¸ªæœ€å°ç¤ºä¾‹ï¼Œé¿å…ç©ºæ–‡ä»¶
        if not clean_comments:
            clean_comments = [
                {"bv": bv, "text": "ç¤ºä¾‹è¯„è®ºï¼šç”±äºæ¥å£é™åˆ¶ï¼Œç”Ÿæˆå ä½æ•°æ®"}
            ]
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        out_dir = os.path.dirname(os.path.abspath(args.out))
        if out_dir and not os.path.exists(out_dir):
            os.makedirs(out_dir, exist_ok=True)
        with open(args.out, "w", encoding="utf-8") as f:
            json.dump(clean_comments, f, ensure_ascii=False, indent=2)
        print(f"Spider OK: bv={bv}, written {len(clean_comments)} comments to {args.out}")
        sys.exit(0)

    # äº¤äº’æ¨¡å¼
    domain_name = input("è¯·è¾“å…¥é¢†åŸŸåç§°(å¦‚: è¶³çƒ)ï¼š").strip() or "default"
    url = input("è¯·è¾“å…¥Bç«™è§†é¢‘URLï¼š").strip()
    if not url:
        print("âŒ URLä¸èƒ½ä¸ºç©ºï¼", file=sys.stderr)
        sys.exit(1)
    bv = extract_bv(url)
    if not bv:
        print(f"âŒ æ— æ³•æå–BVï¼š{url}", file=sys.stderr)
        sys.exit(1)
    success = scrape_single_video(bv, domain_name, headers)
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()